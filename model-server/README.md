# DeepKnit Tensorflow

This repository contains the data and scripts for preprocessing, training and serving the DeepKnit project.

## Technical architecture

All the code for data processing is written in Python. The recommended version to be used is Python 3.6.3. The models
are trained and inferred with [Tensorflow](https://www.tensorflow.org) preferably with GPU support. The high-level API 
[Keras](https://www.tensorflow.org/guide/keras) is used. The model can be used through a simple web server built using
[Flask](http://flask.pocoo.org). The frontend shown in the demo can be found in a separate repository.

The results shown in the DeepKnit Frontend require 3 steps:

1. **Preprocess the raw data**  
   Shima Seiki provided the training data in their proprietary dat format. The byte data in these files needs to be 
   extacted and cleand up to be more meaningful for the LSTM. The header information needs to be removed and the ~200 
   files should be concatenated into a single training file.
   
2. **Train an LSTM model on the training data**  
   A multi layer LSTM is trained onto the previously generated training data. The implementation is roughly based on 
   the keras text generation example [here](https://www.tensorflow.org/tutorials/sequences/text_generation).
   
3. **Serve the trained model**  
   The trained model is accessible using a simple Flask-based web server.
   
Once the model is trained, the architecture of the running demo application looks like this:

```text
+----------------------------+
|      Angular Frontend      |
+----------------------------+
             | ^
HTTP-Request | | Chunked HTTP-Response
             v |
+----------------------------+
|        Flask Server        |
+----------------------------+
             | ^
 Python call | | Python response
             v |
+----------------------------+
|    Tensorflow Backend      |
+----------------------------+
              |
  File access |
              v
+----------------------------+
|       Trained Model        |
+----------------------------+
```

## Reproducing the results

All the scripts should be run from the `src` directory of this repository. Following the steps below should yields to a
properly trained model that can be evaluated from the DeepKnit Frontend.

#### Install dependencies

Install the project dependencies by running `pip3 install -r requirements.txt`.

#### Generate the training files

Based on the raw data found in the repository the training file can be generated by running 
`python3 lstm.py generate-training-file`.

#### Train the LSTM Model

The training can be started by running `python3 lstm.py train`. In our tests 25 epochs gave best results, so the
training can be stopped after that by hitting Ctr+C. Using a GPU for the training is highly recommended to achieve
reasonable speeds.

A pretrained model can be found in the releases tab of Github. This model was trained on a GPU and should also be
executed with a GPU to give meaningful results since different Keras layers are used for CPU and GPU (see 
[here](https://github.com/keras-team/keras/issues/9463)).

#### Build the frontend

Instructions on how to build the frontend can be found in the corresponding repository. The generated files should be
placed in a folder called `static`.

#### Serve the trained model

Start the server by running `python3 server.py`. Open a browser and go to [http://localhost:5000](http://localhost:5000).

## Beyond reproduction

For our research we created a few additional scripts to handle dat files and training results, so looking at the
following scrips might be helpful when working from our results.

#### KnitPaintFileHandler

KnitPaintFileHandler has various methods to read KnitPaint data from different sources, modify and normalize it,
generate previews and export it.

#### Generator Scripts

Some of the functionality of KnitPaintFileHandler is available using a command line interface from `generate.py`. When
using the DeepKnit Frontend especially `python3 generate.py dat-from-image <input> <output>` might be useful to export
generated images back to the dat format.